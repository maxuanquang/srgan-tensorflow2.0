{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy, multiprocessing\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "from model import get_G, get_D\n",
    "from config import config\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-467de74055f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m###====================== HYPER-PARAMETERS ===========================###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m## Adam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m  \u001b[1;31m# use 8 if your GPU memory is small, and change [4, 4] in tl.vis.save_images to [2, 4]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mlr_init\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "###====================== HYPER-PARAMETERS ===========================###\n",
    "## Adam\n",
    "batch_size = config.TRAIN.batch_size  # use 8 if your GPU memory is small, and change [4, 4] in tl.vis.save_images to [2, 4]\n",
    "lr_init = config.TRAIN.lr_init\n",
    "beta1 = config.TRAIN.beta1\n",
    "## initialize G\n",
    "n_epoch_init = config.TRAIN.n_epoch_init\n",
    "## adversarial learning (SRGAN)\n",
    "n_epoch = config.TRAIN.n_epoch\n",
    "lr_decay = config.TRAIN.lr_decay\n",
    "decay_every = config.TRAIN.decay_every\n",
    "shuffle_buffer_size = 128\n",
    "number_of_images = 2\n",
    "\n",
    "# ni = int(np.sqrt(batch_size))\n",
    "\n",
    "# create folders to save result images and trained models\n",
    "save_dir = \"samples\"\n",
    "tl.files.exists_or_mkdir(save_dir)\n",
    "checkpoint_dir = \"models\"\n",
    "tl.files.exists_or_mkdir(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data():\n",
    "    # load dataset\n",
    "    train_hr_img_list = sorted(tl.files.load_file_list(path=config.TRAIN.hr_img_path, regx='.*.png', printable=False))[0:number_of_images]\n",
    "        # train_lr_img_list = sorted(tl.files.load_file_list(path=config.TRAIN.lr_img_path, regx='.*.png', printable=False))\n",
    "        # valid_hr_img_list = sorted(tl.files.load_file_list(path=config.VALID.hr_img_path, regx='.*.png', printable=False))\n",
    "        # valid_lr_img_list = sorted(tl.files.load_file_list(path=config.VALID.lr_img_path, regx='.*.png', printable=False))\n",
    "\n",
    "    ## If your machine have enough memory, please pre-load the entire train set.\n",
    "    train_hr_imgs = tl.vis.read_images(train_hr_img_list, path=config.TRAIN.hr_img_path, n_threads=32)\n",
    "        # for im in train_hr_imgs:\n",
    "        #     print(im.shape)\n",
    "        # valid_lr_imgs = tl.vis.read_images(valid_lr_img_list, path=config.VALID.lr_img_path, n_threads=32)\n",
    "        # for im in valid_lr_imgs:\n",
    "        #     print(im.shape)\n",
    "        # valid_hr_imgs = tl.vis.read_images(valid_hr_img_list, path=config.VALID.hr_img_path, n_threads=32)\n",
    "        # for im in valid_hr_imgs:\n",
    "        #     print(im.shape)\n",
    "        \n",
    "    # dataset API and augmentation\n",
    "    def generator_train():\n",
    "        for img in train_hr_imgs:\n",
    "            yield img\n",
    "    def _map_fn_train(img):\n",
    "        hr_patch = tf.image.random_crop(img, [384, 384, 3])\n",
    "        hr_patch = hr_patch / (255. / 2.)\n",
    "        hr_patch = hr_patch - 1.\n",
    "        hr_patch = tf.image.random_flip_left_right(hr_patch)\n",
    "        lr_patch = tf.image.resize(hr_patch, size=[96, 96])\n",
    "        return lr_patch, hr_patch\n",
    "    train_ds = tf.data.Dataset.from_generator(generator_train, output_types=(tf.float32))\n",
    "    train_ds = train_ds.map(_map_fn_train, num_parallel_calls=multiprocessing.cpu_count())\n",
    "        # train_ds = train_ds.repeat(n_epoch_init + n_epoch)\n",
    "    train_ds = train_ds.shuffle(shuffle_buffer_size)\n",
    "    train_ds = train_ds.prefetch(buffer_size=2)\n",
    "    train_ds = train_ds.batch(batch_size)\n",
    "        # value = train_ds.make_one_shot_iterator().get_next()\n",
    "    return train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    G = get_G((96, 96, 3))\n",
    "    D = get_D((384, 384, 3))\n",
    "#     VGG = tl.models.vgg16(pretrained=True, end_with='pool4', mode='static')\n",
    "    VGG = tf.keras.applications.VGG16(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_tensor=None,\n",
    "        input_shape=None,\n",
    "        pooling=max,\n",
    "        classes=2,\n",
    "#         classifier_activation=\"softmax\",\n",
    "    )\n",
    "\n",
    "    lr_v = tf.Variable(lr_init)\n",
    "    g_optimizer_init = tf.optimizers.Adam(lr_v, beta_1=beta1)\n",
    "    g_optimizer = tf.optimizers.Adam(lr_v, beta_1=beta1)\n",
    "    d_optimizer = tf.optimizers.Adam(lr_v, beta_1=beta1)\n",
    "\n",
    "#     G.train()\n",
    "#     D.train()\n",
    "#     VGG.train()\n",
    "\n",
    "    train_ds = get_train_data()\n",
    "\n",
    "    ## initialize learning (G)\n",
    "    n_step_epoch = round(n_epoch_init // batch_size)\n",
    "    for epoch in range(n_epoch_init):\n",
    "        for step, (lr_patchs, hr_patchs) in enumerate(train_ds):\n",
    "            if lr_patchs.shape[0] != batch_size: # if the remaining data in this epoch < batch_size\n",
    "                break\n",
    "            step_time = time.time()\n",
    "            with tf.GradientTape() as tape:\n",
    "                fake_hr_patchs = G(lr_patchs)\n",
    "                mse_loss = tl.cost.mean_squared_error(fake_hr_patchs, hr_patchs, is_mean=True)\n",
    "            grad = tape.gradient(mse_loss, G.trainable_weights)\n",
    "            g_optimizer_init.apply_gradients(zip(grad, G.trainable_weights))\n",
    "            print(\"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, mse: {:.3f} \".format(\n",
    "                epoch, n_epoch_init, step, n_step_epoch, time.time() - step_time, mse_loss))\n",
    "        if (epoch != 0) and (epoch % 10 == 0):\n",
    "            tl.vis.save_images(fake_hr_patchs.numpy(), [2, 4], os.path.join(save_dir, 'train_g_init_{}.png'.format(epoch)))\n",
    "\n",
    "    ## adversarial learning (G, D)\n",
    "    n_step_epoch = round(n_epoch // batch_size)\n",
    "    for epoch in range(n_epoch):\n",
    "        for step, (lr_patchs, hr_patchs) in enumerate(train_ds):\n",
    "            if lr_patchs.shape[0] != batch_size: # if the remaining data in this epoch < batch_size\n",
    "                break\n",
    "            step_time = time.time()\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                fake_patchs = G(lr_patchs)\n",
    "                logits_fake = D(fake_patchs)\n",
    "                logits_real = D(hr_patchs)\n",
    "                feature_fake = VGG((fake_patchs+1)/2.) # the pre-trained VGG uses the input range of [0, 1]\n",
    "                feature_real = VGG((hr_patchs+1)/2.)\n",
    "                \n",
    "                d_loss1 = tl.cost.sigmoid_cross_entropy(logits_real, tf.ones_like(logits_real))\n",
    "                d_loss2 = tl.cost.sigmoid_cross_entropy(logits_fake, tf.zeros_like(logits_fake))\n",
    "                d_loss = d_loss1 + d_loss2\n",
    "                g_gan_loss = 1e-3 * tl.cost.sigmoid_cross_entropy(logits_fake, tf.ones_like(logits_fake))\n",
    "                mse_loss = tl.cost.mean_squared_error(fake_patchs, hr_patchs, is_mean=True)\n",
    "                vgg_loss = 2e-6 * tl.cost.mean_squared_error(feature_fake, feature_real, is_mean=True)\n",
    "                g_loss = mse_loss + vgg_loss + g_gan_loss\n",
    "            grad = tape.gradient(g_loss, G.trainable_weights)\n",
    "            g_optimizer.apply_gradients(zip(grad, G.trainable_weights))\n",
    "            grad = tape.gradient(d_loss, D.trainable_weights)\n",
    "            d_optimizer.apply_gradients(zip(grad, D.trainable_weights))\n",
    "            print(\"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, g_loss(mse:{:.3f}, vgg:{:.3f}, adv:{:.3f}) d_loss: {:.3f}\".format(\n",
    "                epoch, n_epoch_init, step, n_step_epoch, time.time() - step_time, mse_loss, vgg_loss, g_gan_loss, d_loss))\n",
    "\n",
    "        # update the learning rate\n",
    "        if epoch != 0 and (epoch % decay_every == 0):\n",
    "            new_lr_decay = lr_decay**(epoch // decay_every)\n",
    "            lr_v.assign(lr_init * new_lr_decay)\n",
    "            log = \" ** new learning rate: %f (for GAN)\" % (lr_init * new_lr_decay)\n",
    "            print(log)\n",
    "\n",
    "        if (epoch != 0) and (epoch % 1 == 0):\n",
    "            tl.vis.save_images(fake_patchs.numpy(), [2, 4], os.path.join(save_dir, 'train_g_{}.png'.format(epoch)))\n",
    "            G.save_weights(os.path.join(checkpoint_dir, 'g.h5'))\n",
    "            D.save_weights(os.path.join(checkpoint_dir, 'd.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    ###====================== PRE-LOAD DATA ===========================###\n",
    "    # train_hr_img_list = sorted(tl.files.load_file_list(path=config.TRAIN.hr_img_path, regx='.*.png', printable=False))\n",
    "    # train_lr_img_list = sorted(tl.files.load_file_list(path=config.TRAIN.lr_img_path, regx='.*.png', printable=False))\n",
    "    valid_hr_img_list = sorted(tl.files.load_file_list(path=config.VALID.hr_img_path, regx='.*.png', printable=False))\n",
    "    valid_lr_img_list = sorted(tl.files.load_file_list(path=config.VALID.lr_img_path, regx='.*.png', printable=False))\n",
    "\n",
    "    ## if your machine have enough memory, please pre-load the whole train set.\n",
    "    # train_hr_imgs = tl.vis.read_images(train_hr_img_list, path=config.TRAIN.hr_img_path, n_threads=32)\n",
    "    # for im in train_hr_imgs:\n",
    "    #     print(im.shape)\n",
    "    valid_lr_imgs = tl.vis.read_images(valid_lr_img_list, path=config.VALID.lr_img_path, n_threads=32)\n",
    "    # for im in valid_lr_imgs:\n",
    "    #     print(im.shape)\n",
    "    valid_hr_imgs = tl.vis.read_images(valid_hr_img_list, path=config.VALID.hr_img_path, n_threads=32)\n",
    "    # for im in valid_hr_imgs:\n",
    "    #     print(im.shape)\n",
    "\n",
    "    ###========================== DEFINE MODEL ============================###\n",
    "    imid = 1  # 0: 企鹅  81: 蝴蝶 53: 鸟  64: 古堡\n",
    "    valid_lr_img = valid_lr_imgs[imid]\n",
    "    valid_hr_img = valid_hr_imgs[imid]\n",
    "    # valid_lr_img = get_imgs_fn('test.png', 'data2017/')  # if you want to test your own image\n",
    "    valid_lr_img = (valid_lr_img / 127.5) - 1  # rescale to ［－1, 1]\n",
    "    # print(valid_lr_img.min(), valid_lr_img.max())\n",
    "\n",
    "    G = get_G([None, None, 3])\n",
    "    G.load_weights(os.path.join(checkpoint_dir, 'g.h5'))\n",
    "#     G.eval()\n",
    "\n",
    "    valid_lr_img = np.asarray(valid_lr_img, dtype=np.float32)\n",
    "    valid_lr_img = valid_lr_img[np.newaxis,:,:,:]\n",
    "    size = [valid_lr_img.shape[1], valid_lr_img.shape[2]]\n",
    "\n",
    "    out = G(valid_lr_img).numpy()\n",
    "\n",
    "    print(\"LR size: %s /  generated HR size: %s\" % (size, out.shape))  # LR size: (339, 510, 3) /  gen HR size: (1, 1356, 2040, 3)\n",
    "    print(\"[*] save images\")\n",
    "    tl.vis.save_image(out[0], os.path.join(save_dir, 'valid_gen.png'))\n",
    "    tl.vis.save_image(valid_lr_img[0], os.path.join(save_dir, 'valid_lr.png'))\n",
    "    tl.vis.save_image(valid_hr_img, os.path.join(save_dir, 'valid_hr.png'))\n",
    "\n",
    "#     out_bicu = scipy.misc.imresize(valid_lr_img[0], [size[0] * 4, size[1] * 4], interp='bicubic', mode=None)\n",
    "    out_bicu = cv2.resize(valid_lr_img[0], (size[0] * 4, size[1] * 4))\n",
    "    tl.vis.save_image(out_bicu, os.path.join(save_dir, 'valid_bicubic.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     import argparse\n",
    "#     parser = argparse.ArgumentParser()\n",
    "\n",
    "#     parser.add_argument('--mode', type=str, default='srgan', help='srgan, evaluate')\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     tl.global_flag['mode'] = args.mode\n",
    "\n",
    "#     if tl.global_flag['mode'] == 'srgan':\n",
    "#         train()\n",
    "#     elif tl.global_flag['mode'] == 'evaluate':\n",
    "#         evaluate()\n",
    "#     else:\n",
    "#         raise Exception(\"Unknow --mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] read 2 from DIV2K/DIV2K_train_HR/\n",
      "Epoch: [0/2] step: [0/2] time: 2.049s, mse: 0.585 \n",
      "Epoch: [0/2] step: [1/2] time: 1.870s, mse: 1.848 \n",
      "Epoch: [1/2] step: [0/2] time: 1.841s, mse: 0.353 \n",
      "Epoch: [1/2] step: [1/2] time: 1.857s, mse: 1.093 \n",
      "Epoch: [0/2] step: [0/2] time: 9.014s, g_loss(mse:1.067, vgg:0.000, adv:0.001) d_loss: 1.389\n",
      "Epoch: [0/2] step: [1/2] time: 8.831s, g_loss(mse:0.209, vgg:0.000, adv:0.001) d_loss: 1.366\n",
      "Epoch: [1/2] step: [0/2] time: 8.709s, g_loss(mse:0.192, vgg:0.000, adv:0.001) d_loss: 1.349\n",
      "Epoch: [1/2] step: [1/2] time: 8.851s, g_loss(mse:0.403, vgg:0.000, adv:0.001) d_loss: 1.062\n",
      " ** new learning rate: 0.000010 (for GAN)\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] read 3 from DIV2K/DIV2K_valid_LR_bicubic/X4/\n",
      "[TL] read 3 from DIV2K/DIV2K_valid_HR/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float32 to uint8. Range [-0.6293075084686279, 0.6044202446937561]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR size: [339, 510] /  generated HR size: (1, 1356, 2040, 3)\n",
      "[*] save images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float32 to uint8. Range [-1.0, 1.0]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float32 to uint8. Range [-1.0, 1.0]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
